# LLM Data Capability Spec

## ADDED Requirements

### Requirement: Three-Layer Cache Architecture
The system SHALL store raw data from each source separately and derive a merged view.

#### Scenario: Raw AA data stored independently
- **WHEN** the CLI fetches LLM data from AA API
- **THEN** raw AA data SHALL be stored in `aa_llms.parquet`
- **AND** this file SHALL contain only AA-sourced fields (benchmarks, pricing, performance)

#### Scenario: Raw models.dev data stored independently
- **WHEN** the CLI fetches data from models.dev API
- **THEN** raw models.dev data SHALL be stored in `models_dev.parquet`
- **AND** this file SHALL contain only models.dev-sourced fields (capabilities, limits, modalities)

#### Scenario: Merged view derived from raw sources
- **GIVEN** both `aa_llms.parquet` and `models_dev.parquet` exist
- **WHEN** either source is updated
- **THEN** `llms.parquet` SHALL be regenerated by merging both sources
- **AND** the merged view SHALL contain fields from both sources

### Requirement: Model Capability Data Integration
The system SHALL merge capability metadata from models.dev with benchmark data from Artificial Analysis to provide comprehensive model information.

#### Scenario: Successful data merge
- **GIVEN** AA returns models including "gpt-4o" from creator "openai"
- **AND** models.dev returns provider "openai" with model "gpt-4o"
- **WHEN** the CLI fetches LLM data
- **THEN** the resulting model SHALL include AA benchmarks (intelligence, coding, math)
- **AND** the resulting model SHALL include models.dev capabilities (reasoning, tool_call, context_window)

#### Scenario: Model with no models.dev match
- **GIVEN** AA returns a model with no corresponding entry in models.dev
- **WHEN** the CLI fetches LLM data
- **THEN** the model SHALL display AA data normally
- **AND** capability fields SHALL be null/empty
- **AND** no error SHALL be raised

#### Scenario: models.dev unavailable
- **GIVEN** models.dev API is unreachable or returns an error
- **WHEN** the CLI fetches LLM data
- **THEN** the CLI SHALL display AA data only
- **AND** a warning SHALL be logged (not displayed to user unless verbose)
- **AND** capability fields SHALL be null/empty

### Requirement: Capability Fields
The system SHALL expose the following capability fields for LLM models:

| Field | Type | Description |
|-------|------|-------------|
| reasoning | bool? | Supports reasoning/chain-of-thought |
| tool_call | bool? | Supports tool/function calling |
| structured_output | bool? | Supports structured JSON output |
| attachment | bool? | Supports file attachments |
| temperature | bool? | Supports temperature parameter |
| context_window | int? | Maximum context window (tokens) |
| max_input_tokens | int? | Maximum input tokens |
| max_output_tokens | int? | Maximum output tokens |
| input_modalities | string[]? | Supported input types (text, image, pdf) |
| output_modalities | string[]? | Supported output types (text) |
| knowledge_cutoff | string? | Training data cutoff date |
| open_weights | bool? | Model weights are publicly available |

#### Scenario: Display capability fields in table output
- **WHEN** the user runs `aa llms`
- **THEN** capability columns SHALL be visible in the output
- **AND** boolean fields SHALL display as checkmarks or X (e.g., `+` / `-` / `?`)

#### Scenario: Display capability fields in JSON output
- **WHEN** the user runs `aa llms --format json`
- **THEN** capability fields SHALL be included in JSON output
- **AND** null values SHALL be represented as JSON null

### Requirement: Capability Filtering
The system SHALL support filtering LLMs by capability fields.

#### Scenario: Filter by reasoning support
- **WHEN** the user runs `aa llms --reasoning`
- **THEN** only models with `reasoning = true` SHALL be displayed

#### Scenario: Filter by tool calling support
- **WHEN** the user runs `aa llms --tool-call`
- **THEN** only models with `tool_call = true` SHALL be displayed

#### Scenario: Filter by structured output support
- **WHEN** the user runs `aa llms --structured-output`
- **THEN** only models with `structured_output = true` SHALL be displayed

#### Scenario: Filter by attachment support
- **WHEN** the user runs `aa llms --attachment`
- **THEN** only models with `attachment = true` SHALL be displayed

#### Scenario: Filter by minimum context window
- **WHEN** the user runs `aa llms --min-context 100000`
- **THEN** only models with `context_window >= 100000` SHALL be displayed

#### Scenario: Filter by input modality
- **WHEN** the user runs `aa llms --modality input:image`
- **THEN** only models with "image" in `input_modalities` SHALL be displayed

#### Scenario: Combine capability filters
- **WHEN** the user runs `aa llms --reasoning --tool-call --min-context 128000`
- **THEN** only models matching ALL criteria SHALL be displayed

#### Scenario: Filter with unknown capability
- **GIVEN** a model has `reasoning = null` (unknown)
- **WHEN** the user runs `aa llms --reasoning`
- **THEN** that model SHALL NOT be included in results
- **AND** the filter SHALL only include models with explicit `true` value

### Requirement: Models.dev Caching
The system SHALL cache models.dev data to minimize API requests and support the three-layer architecture.

#### Scenario: Initial fetch
- **GIVEN** no models.dev cache exists
- **WHEN** the CLI fetches LLM data
- **THEN** models.dev data SHALL be fetched from `https://models.dev/api.json`
- **AND** the data SHALL be stored in `models_dev.parquet`

#### Scenario: Cache hit within TTL
- **GIVEN** `models_dev.parquet` exists and is less than 24 hours old
- **WHEN** the CLI fetches LLM data
- **THEN** cached models.dev data SHALL be used
- **AND** no API request to models.dev SHALL be made

#### Scenario: Cache expired
- **GIVEN** `models_dev.parquet` exists but is older than 24 hours
- **WHEN** the CLI fetches LLM data
- **THEN** fresh data SHALL be fetched from models.dev
- **AND** `models_dev.parquet` SHALL be updated
- **AND** `llms.parquet` SHALL be regenerated

#### Scenario: Cache refresh on error with fallback
- **GIVEN** `models_dev.parquet` exists but is expired
- **AND** models.dev API returns an error
- **WHEN** the CLI fetches LLM data
- **THEN** the stale `models_dev.parquet` SHALL be used as fallback
- **AND** a warning SHALL be logged
- **AND** `llms.parquet` SHALL NOT be regenerated

### Requirement: Model Matching Algorithm
The system SHALL match AA models to models.dev entries using a deterministic algorithm.

#### Scenario: Exact composite key match
- **GIVEN** AA model has slug "gpt-4o" and creator slug "openai"
- **AND** models.dev has provider "openai" with model "gpt-4o"
- **WHEN** matching is performed
- **THEN** the models SHALL be matched

#### Scenario: Case-insensitive matching
- **GIVEN** AA model has slug "GPT-4o" and creator slug "OpenAI"
- **AND** models.dev has provider "openai" with model "gpt-4o"
- **WHEN** matching is performed
- **THEN** the models SHALL be matched (case-insensitive)

#### Scenario: Provider name normalization
- **GIVEN** AA model has creator slug "meta"
- **AND** models.dev uses provider "llama" for Meta models
- **WHEN** matching is performed
- **THEN** the provider mapping SHALL be applied
- **AND** matching SHALL succeed

#### Scenario: Versioned model matching
- **GIVEN** AA model has slug "claude-3-5-sonnet"
- **AND** models.dev has model "claude-3-5-sonnet-20241022"
- **WHEN** exact matching fails
- **THEN** fuzzy matching SHALL strip version suffixes
- **AND** the models SHALL be matched
